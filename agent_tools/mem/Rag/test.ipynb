{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "ac4cfbf3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\n",
      "[🔧] Processing AAPL\n",
      "[✅] RAG text saved to ../../../datasets/processed/AAPL/rag_text.csv\n",
      "\n",
      "[🔧] Processing TSLA\n",
      "[✅] RAG text saved to ../../../datasets/processed/TSLA/rag_text.csv\n",
      "\n",
      "[🔧] Processing GOOG\n",
      "[✅] RAG text saved to ../../../datasets/processed/GOOG/rag_text.csv\n",
      "\n",
      "[🔧] Processing MSFT\n",
      "[✅] RAG text saved to ../../../datasets/processed/MSFT/rag_text.csv\n",
      "\n",
      "[🔧] Processing AMZN\n",
      "[✅] RAG text saved to ../../../datasets/processed/AMZN/rag_text.csv\n",
      "\n",
      "[🔧] Processing NFLX\n",
      "[✅] RAG text saved to ../../../datasets/processed/NFLX/rag_text.csv\n",
      "\n",
      "[🔧] Processing NIO\n",
      "[✅] RAG text saved to ../../../datasets/processed/NIO/rag_text.csv\n",
      "\n",
      "[🔧] Processing COIN\n",
      "[✅] RAG text saved to ../../../datasets/processed/COIN/rag_text.csv\n"
     ]
    }
   ],
   "source": [
    "# File: generate_rag_text_all.py\n",
    "import os\n",
    "import re\n",
    "import pandas as pd\n",
    "\n",
    "base_path = \"../../../datasets/processed\"\n",
    "tickers = [\"AAPL\", \"TSLA\", \"GOOG\", \"MSFT\", \"AMZN\", \"NFLX\", \"NIO\", \"COIN\"]\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"\\n[🔧] Processing {ticker}\")\n",
    "    data_path = os.path.join(base_path, ticker, \"financial_with_news_macro_summary.csv\")\n",
    "    agent_path = os.path.join(base_path, ticker, \"cio_analysis_results.csv\")\n",
    "\n",
    "    if not os.path.exists(data_path) or not os.path.exists(agent_path):\n",
    "        print(f\"[❌] Missing files for {ticker}, skipping.\")\n",
    "        continue\n",
    "\n",
    "    df_original = pd.read_csv(data_path)\n",
    "    df_agent = pd.read_csv(agent_path)\n",
    "\n",
    "    df_original[\"date\"] = pd.to_datetime(df_original[\"date\"]).dt.date\n",
    "    df_agent[\"date\"] = pd.to_datetime(df_agent[\"date\"]).dt.date\n",
    "    df_agent = df_agent[[\"date\", \"reasoning\"]]\n",
    "\n",
    "    df_merged = pd.merge(df_original, df_agent, on=\"date\", how=\"inner\")\n",
    "\n",
    "    new_rows = []\n",
    "    for _, row in df_merged.iterrows():\n",
    "        data = row.to_dict()\n",
    "        pattern = re.compile(\n",
    "            r\"\\[(?P<role>.*?) Analyst\\]: \\(action: (?P<action>\\w+)\\) (?P<reasoning>.*?)\\(Confidence: (?P<confidence>[\\d.]+)\\)\",\n",
    "            re.DOTALL,\n",
    "        )\n",
    "        results = pattern.findall(data[\"reasoning\"])\n",
    "\n",
    "        fields = {}\n",
    "        for role, action, reasoning, confidence in results:\n",
    "            key = role.strip().lower().replace(\" \", \"_\")\n",
    "            fields[f\"{key}_action\"] = action.strip()\n",
    "            fields[f\"{key}_confidence\"] = float(confidence.strip())\n",
    "            fields[f\"{key}_reasoning\"] = reasoning.strip()\n",
    "\n",
    "        combined = {**data, **fields}\n",
    "        new_rows.append(combined)\n",
    "\n",
    "    df_with_roles = pd.DataFrame(new_rows)\n",
    "\n",
    "    def build_rag_text(row):\n",
    "        return f\"\"\"\n",
    "Date: {row['date']} | Ticker: {ticker}\n",
    "\n",
    "[Technical Analyst Reasoning]: {row.get('technical_reasoning', '')}\n",
    "[Sentiment Analyst Reasoning]: {row.get('sentiment_reasoning', '')}\n",
    "[Macro Analyst Reasoning]: {row.get('macro_reasoning', '')}\n",
    "[Risk Analyst Reasoning]: {row.get('risk_reasoning', '')}\n",
    "\n",
    "[Market Info]:\n",
    "- Open: {row.get('open', '')}, High: {row.get('high', '')}, Low: {row.get('low', '')}, Close: {row.get('close', '')}\n",
    "- Volume: {row.get('volume', '')}, VIX: {row.get('vix', '')}, Turbulence: {row.get('turbulence', '')}\n",
    "- Sentiment Summary: {row.get('news_summary', '')}\n",
    "- Macro Summary: {row.get('macro_summary', '')}\n",
    "- Risk Tag: {row.get('risk_tag', '')}\n",
    "        \"\"\".strip()\n",
    "\n",
    "    df_with_roles[\"rag_text\"] = df_with_roles.apply(build_rag_text, axis=1)\n",
    "    save_path = os.path.join(base_path, ticker, \"rag_text.csv\")\n",
    "    df_with_roles.to_csv(save_path, index=False)\n",
    "    print(f\"[✅] RAG text saved to {save_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "81ef4b3c",
   "metadata": {},
   "outputs": [
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[2], line 7\u001b[0m\n\u001b[1;32m      5\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtime\u001b[39;00m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtqdm\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m tqdm\n\u001b[0;32m----> 7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mopenai\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m OpenAI\n\u001b[1;32m      8\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mmy_config\u001b[39;00m\n\u001b[1;32m     10\u001b[0m \u001b[38;5;66;03m# 初始化 OpenAI 客户端\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/openai/__init__.py:9\u001b[0m\n\u001b[1;32m      6\u001b[0m \u001b[38;5;28;01mimport\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mas\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m_t\u001b[39;00m\n\u001b[1;32m      7\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mtyping_extensions\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m override\n\u001b[0;32m----> 9\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m types\n\u001b[1;32m     10\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_types\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m NOT_GIVEN, Omit, NoneType, NotGiven, Transport, ProxiesTypes\n\u001b[1;32m     11\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01m_utils\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m file_from_path\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/openai/types/__init__.py:28\u001b[0m\n\u001b[1;32m     26\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mchat_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m ChatModel \u001b[38;5;28;01mas\u001b[39;00m ChatModel\n\u001b[1;32m     27\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mcompletion\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Completion \u001b[38;5;28;01mas\u001b[39;00m Completion\n\u001b[0;32m---> 28\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mmoderation\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m Moderation \u001b[38;5;28;01mas\u001b[39;00m Moderation\n\u001b[1;32m     29\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01maudio_model\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m AudioModel \u001b[38;5;28;01mas\u001b[39;00m AudioModel\n\u001b[1;32m     30\u001b[0m \u001b[38;5;28;01mfrom\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01m.\u001b[39;00m\u001b[38;5;21;01mbatch_error\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;28;01mimport\u001b[39;00m BatchError \u001b[38;5;28;01mas\u001b[39;00m BatchError\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/openai/types/moderation.py:91\u001b[0m\n\u001b[1;32m     87\u001b[0m     violence_graphic: \u001b[38;5;28mbool\u001b[39m \u001b[38;5;241m=\u001b[39m FieldInfo(alias\u001b[38;5;241m=\u001b[39m\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mviolence/graphic\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m     88\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"Content that depicts death, violence, or physical injury in graphic detail.\"\"\"\u001b[39;00m\n\u001b[0;32m---> 91\u001b[0m \u001b[38;5;28;01mclass\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21;01mCategoryAppliedInputTypes\u001b[39;00m(BaseModel):\n\u001b[1;32m     92\u001b[0m     harassment: List[Literal[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mtext\u001b[39m\u001b[38;5;124m\"\u001b[39m]]\n\u001b[1;32m     93\u001b[0m \u001b[38;5;250m    \u001b[39m\u001b[38;5;124;03m\"\"\"The applied input type(s) for the category 'harassment'.\"\"\"\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py:221\u001b[0m, in \u001b[0;36mModelMetaclass.__new__\u001b[0;34m(mcs, cls_name, bases, namespace, __pydantic_generic_metadata__, __pydantic_reset_parent_namespace__, _create_model_module, **kwargs)\u001b[0m\n\u001b[1;32m    217\u001b[0m     parent_namespace \u001b[38;5;241m=\u001b[39m unpack_lenient_weakvaluedict(parent_namespace)\n\u001b[1;32m    219\u001b[0m ns_resolver \u001b[38;5;241m=\u001b[39m NsResolver(parent_namespace\u001b[38;5;241m=\u001b[39mparent_namespace)\n\u001b[0;32m--> 221\u001b[0m \u001b[43mset_model_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mns_resolver\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    223\u001b[0m \u001b[38;5;66;03m# This is also set in `complete_model_class()`, after schema gen because they are recreated.\u001b[39;00m\n\u001b[1;32m    224\u001b[0m \u001b[38;5;66;03m# We set them here as well for backwards compatibility:\u001b[39;00m\n\u001b[1;32m    225\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_computed_fields__ \u001b[38;5;241m=\u001b[39m {\n\u001b[1;32m    226\u001b[0m     k: v\u001b[38;5;241m.\u001b[39minfo \u001b[38;5;28;01mfor\u001b[39;00m k, v \u001b[38;5;129;01min\u001b[39;00m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_decorators__\u001b[38;5;241m.\u001b[39mcomputed_fields\u001b[38;5;241m.\u001b[39mitems()\n\u001b[1;32m    227\u001b[0m }\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/pydantic/_internal/_model_construction.py:544\u001b[0m, in \u001b[0;36mset_model_fields\u001b[0;34m(cls, config_wrapper, ns_resolver)\u001b[0m\n\u001b[1;32m    536\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Collect and set `cls.__pydantic_fields__` and `cls.__class_vars__`.\u001b[39;00m\n\u001b[1;32m    537\u001b[0m \n\u001b[1;32m    538\u001b[0m \u001b[38;5;124;03mArgs:\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    541\u001b[0m \u001b[38;5;124;03m    ns_resolver: Namespace resolver to use when getting model annotations.\u001b[39;00m\n\u001b[1;32m    542\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    543\u001b[0m typevars_map \u001b[38;5;241m=\u001b[39m get_model_typevars_map(\u001b[38;5;28mcls\u001b[39m)\n\u001b[0;32m--> 544\u001b[0m fields, class_vars \u001b[38;5;241m=\u001b[39m \u001b[43mcollect_model_fields\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mcls\u001b[39;49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mconfig_wrapper\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mns_resolver\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mtypevars_map\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mtypevars_map\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    546\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__pydantic_fields__ \u001b[38;5;241m=\u001b[39m fields\n\u001b[1;32m    547\u001b[0m \u001b[38;5;28mcls\u001b[39m\u001b[38;5;241m.\u001b[39m__class_vars__\u001b[38;5;241m.\u001b[39mupdate(class_vars)\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/pydantic/_internal/_fields.py:237\u001b[0m, in \u001b[0;36mcollect_model_fields\u001b[0;34m(cls, config_wrapper, ns_resolver, typevars_map)\u001b[0m\n\u001b[1;32m    232\u001b[0m \u001b[38;5;66;03m# The `from_annotated_attribute()` call below mutates the assigned `Field()`, so make a copy:\u001b[39;00m\n\u001b[1;32m    233\u001b[0m original_assignment \u001b[38;5;241m=\u001b[39m (\n\u001b[1;32m    234\u001b[0m     assigned_value\u001b[38;5;241m.\u001b[39m_copy() \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluated \u001b[38;5;129;01mand\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(assigned_value, FieldInfo_) \u001b[38;5;28;01melse\u001b[39;00m assigned_value\n\u001b[1;32m    235\u001b[0m )\n\u001b[0;32m--> 237\u001b[0m field_info \u001b[38;5;241m=\u001b[39m \u001b[43mFieldInfo_\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mfrom_annotated_attribute\u001b[49m\u001b[43m(\u001b[49m\u001b[43mann_type\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43massigned_value\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43m_source\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mAnnotationSource\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mCLASS\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    238\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m evaluated:\n\u001b[1;32m    239\u001b[0m     field_info\u001b[38;5;241m.\u001b[39m_complete \u001b[38;5;241m=\u001b[39m \u001b[38;5;28;01mFalse\u001b[39;00m\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/pydantic/fields.py:423\u001b[0m, in \u001b[0;36mFieldInfo.from_annotated_attribute\u001b[0;34m(annotation, default, _source)\u001b[0m\n\u001b[1;32m    421\u001b[0m default\u001b[38;5;241m.\u001b[39mannotation \u001b[38;5;241m=\u001b[39m type_expr\n\u001b[1;32m    422\u001b[0m default\u001b[38;5;241m.\u001b[39mmetadata \u001b[38;5;241m+\u001b[39m\u001b[38;5;241m=\u001b[39m metadata\n\u001b[0;32m--> 423\u001b[0m merged_default \u001b[38;5;241m=\u001b[39m \u001b[43mFieldInfo\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mmerge_field_infos\u001b[49m\u001b[43m(\u001b[49m\n\u001b[1;32m    424\u001b[0m \u001b[43m    \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43m[\u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mfor\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mx\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;129;43;01min\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[43mmetadata\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mif\u001b[39;49;00m\u001b[43m \u001b[49m\u001b[38;5;28;43misinstance\u001b[39;49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43mFieldInfo\u001b[49m\u001b[43m)\u001b[49m\u001b[43m]\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    425\u001b[0m \u001b[43m    \u001b[49m\u001b[43mdefault\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    426\u001b[0m \u001b[43m    \u001b[49m\u001b[43mannotation\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43mdefault\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mannotation\u001b[49m\u001b[43m,\u001b[49m\n\u001b[1;32m    427\u001b[0m \u001b[43m\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    428\u001b[0m merged_default\u001b[38;5;241m.\u001b[39mfrozen \u001b[38;5;241m=\u001b[39m final \u001b[38;5;129;01mor\u001b[39;00m merged_default\u001b[38;5;241m.\u001b[39mfrozen\n\u001b[1;32m    429\u001b[0m merged_default\u001b[38;5;241m.\u001b[39m_qualifiers \u001b[38;5;241m=\u001b[39m inspected_ann\u001b[38;5;241m.\u001b[39mqualifiers\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/pydantic/fields.py:481\u001b[0m, in \u001b[0;36mFieldInfo.merge_field_infos\u001b[0;34m(*field_infos, **overrides)\u001b[0m\n\u001b[1;32m    472\u001b[0m \u001b[38;5;250m\u001b[39m\u001b[38;5;124;03m\"\"\"Merge `FieldInfo` instances keeping only explicitly set attributes.\u001b[39;00m\n\u001b[1;32m    473\u001b[0m \n\u001b[1;32m    474\u001b[0m \u001b[38;5;124;03mLater `FieldInfo` instances override earlier ones.\u001b[39;00m\n\u001b[0;32m   (...)\u001b[0m\n\u001b[1;32m    477\u001b[0m \u001b[38;5;124;03m    FieldInfo: A merged FieldInfo instance.\u001b[39;00m\n\u001b[1;32m    478\u001b[0m \u001b[38;5;124;03m\"\"\"\u001b[39;00m\n\u001b[1;32m    479\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28mlen\u001b[39m(field_infos) \u001b[38;5;241m==\u001b[39m \u001b[38;5;241m1\u001b[39m:\n\u001b[1;32m    480\u001b[0m     \u001b[38;5;66;03m# No merging necessary, but we still need to make a copy and apply the overrides\u001b[39;00m\n\u001b[0;32m--> 481\u001b[0m     field_info \u001b[38;5;241m=\u001b[39m \u001b[43mfield_infos\u001b[49m\u001b[43m[\u001b[49m\u001b[38;5;241;43m0\u001b[39;49m\u001b[43m]\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_copy\u001b[49m\u001b[43m(\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m    482\u001b[0m     field_info\u001b[38;5;241m.\u001b[39m_attributes_set\u001b[38;5;241m.\u001b[39mupdate(overrides)\n\u001b[1;32m    484\u001b[0m     default_override \u001b[38;5;241m=\u001b[39m overrides\u001b[38;5;241m.\u001b[39mpop(\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mdefault\u001b[39m\u001b[38;5;124m'\u001b[39m, PydanticUndefined)\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/pydantic/fields.py:592\u001b[0m, in \u001b[0;36mFieldInfo._copy\u001b[0;34m(self)\u001b[0m\n\u001b[1;32m    591\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_copy\u001b[39m(\u001b[38;5;28mself\u001b[39m) \u001b[38;5;241m-\u001b[39m\u001b[38;5;241m>\u001b[39m Self:\n\u001b[0;32m--> 592\u001b[0m     copied \u001b[38;5;241m=\u001b[39m \u001b[43mcopy\u001b[49m\u001b[43m(\u001b[49m\u001b[38;5;28;43mself\u001b[39;49m\u001b[43m)\u001b[49m\n\u001b[1;32m    593\u001b[0m     \u001b[38;5;28;01mfor\u001b[39;00m attr_name \u001b[38;5;129;01min\u001b[39;00m (\u001b[38;5;124m'\u001b[39m\u001b[38;5;124mmetadata\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_attributes_set\u001b[39m\u001b[38;5;124m'\u001b[39m, \u001b[38;5;124m'\u001b[39m\u001b[38;5;124m_qualifiers\u001b[39m\u001b[38;5;124m'\u001b[39m):\n\u001b[1;32m    594\u001b[0m         \u001b[38;5;66;03m# Apply \"deep-copy\" behavior on collections attributes:\u001b[39;00m\n\u001b[1;32m    595\u001b[0m         value \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mgetattr\u001b[39m(copied, attr_name)\u001b[38;5;241m.\u001b[39mcopy()\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/copy.py:102\u001b[0m, in \u001b[0;36mcopy\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    100\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m \u001b[38;5;28misinstance\u001b[39m(rv, \u001b[38;5;28mstr\u001b[39m):\n\u001b[1;32m    101\u001b[0m     \u001b[38;5;28;01mreturn\u001b[39;00m x\n\u001b[0;32m--> 102\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[43m_reconstruct\u001b[49m\u001b[43m(\u001b[49m\u001b[43mx\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;28;43;01mNone\u001b[39;49;00m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[38;5;241;43m*\u001b[39;49m\u001b[43mrv\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/copy.py:259\u001b[0m, in \u001b[0;36m_reconstruct\u001b[0;34m(x, memo, func, args, state, listiter, dictiter, deepcopy)\u001b[0m\n\u001b[1;32m    255\u001b[0m     \u001b[38;5;28;01mexcept\u001b[39;00m \u001b[38;5;167;01mKeyError\u001b[39;00m:\n\u001b[1;32m    256\u001b[0m         \u001b[38;5;66;03m# aha, this is the first one :-)\u001b[39;00m\n\u001b[1;32m    257\u001b[0m         memo[\u001b[38;5;28mid\u001b[39m(memo)]\u001b[38;5;241m=\u001b[39m[x]\n\u001b[0;32m--> 259\u001b[0m \u001b[38;5;28;01mdef\u001b[39;00m\u001b[38;5;250m \u001b[39m\u001b[38;5;21m_reconstruct\u001b[39m(x, memo, func, args,\n\u001b[1;32m    260\u001b[0m                  state\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, listiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m, dictiter\u001b[38;5;241m=\u001b[39m\u001b[38;5;28;01mNone\u001b[39;00m,\n\u001b[1;32m    261\u001b[0m                  \u001b[38;5;241m*\u001b[39m, deepcopy\u001b[38;5;241m=\u001b[39mdeepcopy):\n\u001b[1;32m    262\u001b[0m     deep \u001b[38;5;241m=\u001b[39m memo \u001b[38;5;129;01mis\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m \u001b[38;5;28;01mNone\u001b[39;00m\n\u001b[1;32m    263\u001b[0m     \u001b[38;5;28;01mif\u001b[39;00m deep \u001b[38;5;129;01mand\u001b[39;00m args:\n",
      "\u001b[0;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "# File: embed_rag_text_all.py\n",
    "\n",
    "import os\n",
    "import pandas as pd\n",
    "import time\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import my_config\n",
    "\n",
    "# 初始化 OpenAI 客户端\n",
    "client = OpenAI(\n",
    "    api_key=my_config.api_key,\n",
    "    base_url=\"https://api.openai.com/v1\"\n",
    ")\n",
    "\n",
    "tickers = [\"AAPL\", \"TSLA\", \"GOOG\", \"MSFT\", \"AMZN\", \"NFLX\", \"NIO\", \"COIN\"]\n",
    "base_path = \"../../../datasets/processed\"\n",
    "\n",
    "def get_embedding(text):\n",
    "    if not isinstance(text, str) or not text.strip():\n",
    "        return None  # 跳过空文本\n",
    "    for attempt in range(3):  # 最多重试三次\n",
    "        try:\n",
    "            resp = client.embeddings.create(\n",
    "                model=\"text-embedding-ada-002\",  # 可替换为 text-embedding-3-small\n",
    "                input=text.strip()\n",
    "            )\n",
    "            return resp.data[0].embedding\n",
    "        except Exception as e:\n",
    "            print(f\"[⚠️ Error] Attempt {attempt+1}: {e}\")\n",
    "            time.sleep(2)\n",
    "    return None  # 超过重试次数仍失败则返回空\n",
    "\n",
    "# 遍历每个 ticker 文件夹进行嵌入处理\n",
    "for ticker in tickers:\n",
    "    print(f\"\\n[🔧] Embedding for {ticker}\")\n",
    "    rag_path = os.path.join(base_path, ticker, \"rag_text.csv\")\n",
    "    output_path = os.path.join(base_path, ticker, \"rag_text_with_embedding.csv\")\n",
    "\n",
    "    if not os.path.exists(rag_path):\n",
    "        print(f\"[❌] RAG text not found for {ticker}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(rag_path)\n",
    "\n",
    "    if \"embedding\" not in df.columns:\n",
    "        tqdm.pandas(desc=f\"Embedding {ticker}\")\n",
    "        df[\"embedding\"] = df[\"rag_text\"].progress_apply(get_embedding)\n",
    "        df.dropna(subset=[\"embedding\"], inplace=True)\n",
    "        df.to_csv(output_path, index=False)\n",
    "        print(f\"[✅] Saved to {output_path}\")\n",
    "    else:\n",
    "        print(f\"[⏭️] Already embedded: {output_path}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "297d20b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: rag_inference_all.py\n",
    "import os\n",
    "import json\n",
    "import faiss\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from datetime import timedelta\n",
    "from tqdm import tqdm\n",
    "from openai import OpenAI\n",
    "import my_config\n",
    "from tools import function_schema\n",
    "\n",
    "client = OpenAI(\n",
    "    api_key=my_config.api_key,\n",
    "    base_url=\"https://api.deepseek.com\"\n",
    "    )\n",
    "\n",
    "base_path = \"../../../datasets/processed\"\n",
    "tickers = [\"AAPL\", \"TSLA\", \"GOOG\", \"MSFT\", \"AMZN\", \"NFLX\", \"NIO\", \"COIN\"]\n",
    "\n",
    "def generate_suggestion(target_row, df_range, top_k=5):\n",
    "    query_vec = np.array(eval(target_row[\"embedding\"])).astype(\"float32\").reshape(1, -1)\n",
    "    df_range[\"embedding\"] = df_range[\"embedding\"].apply(eval)\n",
    "\n",
    "    index = faiss.IndexFlatL2(len(df_range[\"embedding\"].iloc[0]))\n",
    "    index.add(np.vstack(df_range[\"embedding\"].values).astype(\"float32\"))\n",
    "\n",
    "    distances, indices = index.search(query_vec, top_k)\n",
    "    similar_contexts = \"\\n\\n\".join(df_range.iloc[i][\"rag_text\"] for i in indices[0])\n",
    "    current_context = target_row[\"rag_text\"]\n",
    "    start_date = df_range[\"date\"].min().strftime(\"%Y-%m-%d\")\n",
    "    end_date = df_range[\"date\"].max().strftime(\"%Y-%m-%d\")\n",
    "\n",
    "    prompt = (\n",
    "        f\"You are acting as a Chief Investment Officer (CIO) at a top-tier financial institution.\\n\"\n",
    "        f\"Your role is to make informed and professional investment decisions based on current market data and historical analogs.\\n\\n\"\n",
    "        f\"**Current Date**: {target_row['date']}\\n\\n\"\n",
    "        f\"**Current Market Context Summary (from RAG text)**:\\n\"\n",
    "        f\"{current_context}\\n\\n\"\n",
    "        f\"**Most Relevant Historical Market Cases (from FAISS-matched RAG texts between {start_date} and {end_date})**:\\n\"\n",
    "        f\"{similar_contexts}\\n\\n\"\n",
    "        f\"Please follow these steps:\\n\"\n",
    "        f\"1. Carefully analyze the **Current Market Context**, extracting key financial signals, trends, risks, or sentiments.\\n\"\n",
    "        f\"2. Compare these elements with the **Historical Cases** provided.\\n\"\n",
    "        f\"3. Identify patterns, analogies, and lessons from history that can help assess the current situation.\\n\"\n",
    "        f\"4. Formulate a reasoned investment decision based on your analysis.\\n\\n\"\n",
    "        f\"Output Format:\\n\"\n",
    "        f\"- `action`: one of ['buy', 'sell', 'hold']\\n\"\n",
    "        f\"- `confidence`: float between 0 and 1, indicating how strongly you support this action\\n\"\n",
    "        f\"- `reason`: a concise but professional justification based on evidence from both current and historical contexts\\n\\n\"\n",
    "        f\"\\\"Respond by calling the 'stock_decision' function according to the schema.\\\"\"\n",
    "    )\n",
    "\n",
    "\n",
    "    response = client.chat.completions.create(\n",
    "        model=\"deepseek-chat\",\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": \"You are a financial CIO giving investment decisions.\"},\n",
    "            {\"role\": \"user\", \"content\": prompt}\n",
    "        ],\n",
    "        tools=[{\n",
    "            \"type\": \"function\",\n",
    "            \"function\": function_schema[0]\n",
    "        }] if function_schema else [],\n",
    "\n",
    "        tool_choice={\"type\": \"function\", \"function\": {\"name\": function_schema[0][\"name\"]}} if function_schema else \"auto\"\n",
    "    )\n",
    "\n",
    "    message = response.choices[0].message\n",
    "\n",
    "    if message.tool_calls:\n",
    "        tool_call = message.tool_calls[0]\n",
    "        args = tool_call.function.arguments\n",
    "        parsed = json.loads(args)\n",
    "        return parsed\n",
    "    else:\n",
    "        print(\"[⚠️ tool_calls 为空，未触发函数调用]\")\n",
    "        return {\n",
    "            \"action\": \"hold\",\n",
    "            \"confidence\": 0.5,\n",
    "            \"reasoning\": \"Model did not return tool call\"\n",
    "        }\n",
    "\n",
    "for ticker in tickers:\n",
    "    print(f\"\\n[🧠] RAG inference for {ticker}\")\n",
    "    df_path = os.path.join(base_path, ticker, \"rag_text_with_embedding.csv\")\n",
    "    output_path = os.path.join(base_path, ticker, \"rag_agent_suggestions.csv\")\n",
    "\n",
    "    if not os.path.exists(df_path):\n",
    "        print(f\"[❌] Missing embedding file: {df_path}\")\n",
    "        continue\n",
    "\n",
    "    df = pd.read_csv(df_path)\n",
    "    df[\"date\"] = pd.to_datetime(df[\"date\"])\n",
    "    all_dates = sorted(df[\"date\"].unique())\n",
    "\n",
    "    train_start = pd.to_datetime(\"2022-01-03\")\n",
    "    train_end = pd.to_datetime(\"2022-10-04\")\n",
    "\n",
    "    results = []\n",
    "\n",
    "    for current_date in tqdm(all_dates):\n",
    "        row = df[df[\"date\"] == current_date]\n",
    "        if row.empty:\n",
    "            continue\n",
    "        row = row.iloc[0]\n",
    "\n",
    "        if current_date <= train_end:\n",
    "            df_range = df[(df[\"date\"] >= train_start) & (df[\"date\"] <= train_end)]\n",
    "        else:\n",
    "            df_range = df[(df[\"date\"] >= train_start) & (df[\"date\"] < current_date)]\n",
    "\n",
    "        if df_range.empty:\n",
    "            print(f\"[⏭️] No context for {current_date.date()}\")\n",
    "            continue\n",
    "\n",
    "        try:\n",
    "            suggestion = generate_suggestion(row, df_range)\n",
    "            results.append({\n",
    "                \"date\": current_date.strftime(\"%Y-%m-%d\"),\n",
    "                **suggestion\n",
    "            })\n",
    "        except Exception as e:\n",
    "            print(f\"[❌] Error on {current_date.date()}: {e}\")\n",
    "            continue\n",
    "\n",
    "    pd.DataFrame(results).to_csv(output_path, index=False)\n",
    "    print(f\"[✅] Saved: {output_path}\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qwen-Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
