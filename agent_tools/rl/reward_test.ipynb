{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 64,
   "id": "88cb0f40",
   "metadata": {},
   "outputs": [],
   "source": [
    "# File: meta_agent_env.py\n",
    "import os\n",
    "import sys\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import gym\n",
    "from gym import spaces\n",
    "\n",
    "# === Add project root to path ===\n",
    "project_root = os.path.abspath(os.path.join(os.getcwd(), '..', '..'))\n",
    "if project_root not in sys.path:\n",
    "    sys.path.insert(0, project_root)\n",
    "\n",
    "\n",
    "def precompute_observations(data, advisors):\n",
    "    extra_features = [\"macd\", \"rsi_30\", \"cci_30\", \"vix\", \"turbulence\", \"close_30_sma\", \"close_60_sma\"]\n",
    "    # extra_features = []\n",
    "    cached_obs = []\n",
    "\n",
    "    for i in range(len(data)):\n",
    "        row = data.iloc[i].to_dict()\n",
    "        obs = []\n",
    "\n",
    "        for name, agent in advisors.items():\n",
    "            result = agent.analyze(row)\n",
    "            action = result[\"action\"]\n",
    "            confidence = result[\"confidence\"]\n",
    "            for act in [\"buy\", \"hold\", \"sell\"]:\n",
    "                obs.append(confidence if act == action else 0.0)\n",
    "\n",
    "        obs.append(0.0)  # holding 状态初始化为 False\n",
    "\n",
    "        for feat in extra_features:\n",
    "            obs.append(row.get(feat, 0.0))\n",
    "\n",
    "        cached_obs.append(np.array(obs, dtype=np.float32))\n",
    "\n",
    "    return cached_obs\n",
    "\n",
    "\n",
    "class MetaCIOEnv(gym.Env):\n",
    "    def __init__(self, data, advisors, cached_obs=None):\n",
    "        super(MetaCIOEnv, self).__init__()\n",
    "\n",
    "        self.data = data.reset_index(drop=True)\n",
    "        self.advisors = advisors\n",
    "        self.cached_obs = cached_obs\n",
    "        self.current_step = 0\n",
    "        self.holding = False\n",
    "        self.entry_price = 0.0\n",
    "        self.holding_days = 0\n",
    "\n",
    "        self.extra_features = [\"macd\", \"rsi_30\", \"cci_30\", \"vix\", \"turbulence\", \"close_30_sma\", \"close_60_sma\"]\n",
    "        # self.extra_features = []\n",
    "        obs_dim = len(advisors) * 3 + 1 + len(self.extra_features)\n",
    "        self.observation_space = spaces.Box(low=-np.inf, high=np.inf, shape=(obs_dim,), dtype=np.float32)\n",
    "\n",
    "        self.action_space = spaces.Discrete(3)  # 0-buy, 1-hold, 2-sell\n",
    "\n",
    "    def _get_obs(self):\n",
    "        if self.cached_obs is not None:\n",
    "            obs = self.cached_obs[self.current_step].copy()\n",
    "\n",
    "            holding_index = len(self.advisors) * 3\n",
    "            obs[holding_index] = 1.0 if self.holding else 0.0\n",
    "            return obs\n",
    "        else:\n",
    "            row = self.data.iloc[self.current_step]\n",
    "            obs = []\n",
    "            for name, agent in self.advisors.items():\n",
    "                result = agent.analyze(row.to_dict())\n",
    "                action = result[\"action\"]\n",
    "                confidence = result[\"confidence\"]\n",
    "                for act in [\"buy\", \"hold\", \"sell\"]:\n",
    "                    obs.append(confidence if act == action else 0.0)\n",
    "\n",
    "            obs.append(1.0 if self.holding else 0.0)\n",
    "            for feat in self.extra_features:\n",
    "                obs.append(row.get(feat, 0.0))\n",
    "\n",
    "            return np.array(obs, dtype=np.float32)\n",
    "\n",
    "    def reset(self):\n",
    "        self.current_step = 0\n",
    "        self.holding = False\n",
    "        self.entry_price = 0.0\n",
    "        self.holding_days = 0\n",
    "        return self._get_obs()\n",
    "\n",
    "    def step(self, action):\n",
    "        row = self.data.iloc[self.current_step]\n",
    "        price = row[\"close\"]\n",
    "\n",
    "        reward = 0.0\n",
    "        transaction_cost = 0.001\n",
    "\n",
    "        if action == 0 and not self.holding:\n",
    "            self.holding = True\n",
    "            self.entry_price = price\n",
    "            self.holding_days = 0\n",
    "            # reward -= price * transaction_cost\n",
    "        elif action == 2 and self.holding:\n",
    "            pnl = (price - self.entry_price) / self.entry_price\n",
    "            reward += pnl\n",
    "            # reward -= price * transaction_cost\n",
    "            self.holding = False\n",
    "            self.entry_price = 0.0\n",
    "            self.holding_days = 0\n",
    "        elif self.holding:\n",
    "            # Holding penalty, no realized pnl\n",
    "            reward -= 0.0005\n",
    "            self.holding_days += 1\n",
    "\n",
    "        self.current_step += 1\n",
    "        done = self.current_step >= len(self.cached_obs) - 1\n",
    "        obs = self._get_obs()\n",
    "\n",
    "        return obs, reward, done, {}\n",
    "\n",
    "    def render(self, mode='human'):\n",
    "        print(f\"Step: {self.current_step}\")\n",
    "\n",
    "\n",
    "from agent_tools.open_ai.agent_roles_openai import (\n",
    "    TechnicalAnalystAgent, SentimentAnalystAgent, MacroAnalystAgent, RiskAnalystAgent\n",
    ")\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "df = pd.read_csv(\"../../datasets/processed/financial_with_news_macro_summary.csv\")\n",
    "\n",
    "advisors = {\n",
    "    \"tech\": TechnicalAnalystAgent(),\n",
    "    \"sent\": SentimentAnalystAgent(),\n",
    "    \"macro\": MacroAnalystAgent(),\n",
    "    \"risk\": RiskAnalystAgent()\n",
    "}\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "id": "432e467b",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Loading cached observations from cached_observations.csv ...\n"
     ]
    }
   ],
   "source": [
    "from agent_tools.open_ai.agent_roles_openai import (\n",
    "        TechnicalAnalystAgent, SentimentAnalystAgent, MacroAnalystAgent, RiskAnalystAgent\n",
    "    )\n",
    "from stable_baselines3 import PPO\n",
    "\n",
    "df = pd.read_csv(\"../../datasets/processed/financial_with_news_macro_summary.csv\")\n",
    "\n",
    "advisors = {\n",
    "    \"tech\": TechnicalAnalystAgent(),\n",
    "    \"sent\": SentimentAnalystAgent(),\n",
    "    \"macro\": MacroAnalystAgent(),\n",
    "    \"risk\": RiskAnalystAgent()\n",
    "}\n",
    "\n",
    "cache_path = \"cached_observations.csv\"\n",
    "\n",
    "if os.path.exists(cache_path):\n",
    "    print(f\"Loading cached observations from {cache_path} ...\")\n",
    "    df_cached = pd.read_csv(cache_path)\n",
    "    cached_obs = df_cached.values.astype(np.float32)\n",
    "else:\n",
    "    print(\"Cached observations not found, precomputing...\")\n",
    "    cached_obs = precompute_observations(df, advisors)\n",
    "    cached_obs_array = np.stack(cached_obs)\n",
    "    pd.DataFrame(cached_obs_array).to_csv(cache_path, index=False)\n",
    "    print(f\"Cached observations saved to {cache_path}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "id": "c0500fc0",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Wrapping the env with a `Monitor` wrapper\n",
      "Wrapping the env in a DummyVecEnv.\n",
      "Step: 1\n",
      "1\n",
      "Step: 2\n",
      "1\n",
      "Step: 3\n",
      "0\n",
      "Step: 4\n",
      "0\n",
      "Step: 5\n",
      "0\n",
      "Step: 6\n",
      "0\n",
      "Step: 7\n",
      "0\n",
      "Step: 8\n",
      "0\n",
      "Step: 9\n",
      "0\n",
      "Step: 10\n",
      "0\n",
      "Step: 11\n",
      "1\n",
      "Step: 12\n",
      "2\n",
      "Step: 13\n",
      "2\n",
      "Step: 14\n",
      "0\n",
      "Step: 15\n",
      "2\n",
      "Step: 16\n",
      "0\n",
      "Step: 17\n",
      "0\n",
      "Step: 18\n",
      "0\n",
      "Step: 19\n",
      "2\n",
      "Step: 20\n",
      "2\n",
      "Step: 21\n",
      "2\n",
      "Step: 22\n",
      "1\n",
      "Step: 23\n",
      "1\n",
      "Step: 24\n",
      "0\n",
      "Step: 25\n",
      "2\n",
      "Step: 26\n",
      "2\n",
      "Step: 27\n",
      "0\n",
      "Step: 28\n",
      "0\n",
      "Step: 29\n",
      "0\n",
      "Step: 30\n",
      "2\n",
      "Step: 31\n",
      "1\n",
      "Step: 32\n",
      "0\n",
      "Step: 33\n",
      "1\n",
      "Step: 34\n",
      "0\n",
      "Step: 35\n",
      "2\n",
      "Step: 36\n",
      "2\n",
      "Step: 37\n",
      "0\n",
      "Step: 38\n",
      "0\n",
      "Step: 39\n",
      "2\n",
      "Step: 40\n",
      "0\n",
      "Step: 41\n",
      "2\n",
      "Step: 42\n",
      "2\n",
      "Step: 43\n",
      "2\n",
      "Step: 44\n",
      "2\n",
      "Step: 45\n",
      "2\n",
      "Step: 46\n",
      "2\n",
      "Step: 47\n",
      "0\n",
      "Step: 48\n",
      "2\n",
      "Step: 49\n",
      "1\n",
      "Step: 50\n",
      "0\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/postgraduates/2024/chenjiarui/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/stable_baselines3/common/vec_env/patch_gym.py:49: UserWarning: You provided an OpenAI Gym environment. We strongly recommend transitioning to Gymnasium environments. Stable-Baselines3 is automatically wrapping your environments in a compatibility layer, which could potentially cause issues.\n",
      "  warnings.warn(\n",
      "/data/postgraduates/2024/chenjiarui/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/stable_baselines3/common/on_policy_algorithm.py:150: UserWarning: You are trying to run PPO on the GPU, but it is primarily intended to run on the CPU when not using a CNN policy (you are using ActorCriticPolicy which should be a MlpPolicy). See https://github.com/DLR-RM/stable-baselines3/issues/1245 for more info. You can pass `device='cpu'` or `export CUDA_VISIBLE_DEVICES=` to force using the CPU.Note: The model will train, but the GPU utilization will be poor and the training might take longer than on CPU.\n",
      "  warnings.warn(\n"
     ]
    },
    {
     "ename": "IndexError",
     "evalue": "single positional indexer is out-of-bounds",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mIndexError\u001b[0m                                Traceback (most recent call last)",
      "Cell \u001b[0;32mIn[66], line 23\u001b[0m\n\u001b[1;32m     20\u001b[0m rewards \u001b[38;5;241m=\u001b[39m []\n\u001b[1;32m     22\u001b[0m \u001b[38;5;28;01mwhile\u001b[39;00m \u001b[38;5;129;01mnot\u001b[39;00m done:\n\u001b[0;32m---> 23\u001b[0m     row \u001b[38;5;241m=\u001b[39m \u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mdata\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43miloc\u001b[49m\u001b[43m[\u001b[49m\u001b[43menv\u001b[49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43mcurrent_step\u001b[49m\u001b[43m]\u001b[49m\n\u001b[1;32m     24\u001b[0m     price \u001b[38;5;241m=\u001b[39m row[\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mclose\u001b[39m\u001b[38;5;124m\"\u001b[39m]\n\u001b[1;32m     26\u001b[0m     action, _ \u001b[38;5;241m=\u001b[39m model\u001b[38;5;241m.\u001b[39mpredict(obs)\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/pandas/core/indexing.py:1191\u001b[0m, in \u001b[0;36m_LocationIndexer.__getitem__\u001b[0;34m(self, key)\u001b[0m\n\u001b[1;32m   1189\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m com\u001b[38;5;241m.\u001b[39mapply_if_callable(key, \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj)\n\u001b[1;32m   1190\u001b[0m maybe_callable \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39m_check_deprecated_callable_usage(key, maybe_callable)\n\u001b[0;32m-> 1191\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_getitem_axis\u001b[49m\u001b[43m(\u001b[49m\u001b[43mmaybe_callable\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[38;5;241;43m=\u001b[39;49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/pandas/core/indexing.py:1752\u001b[0m, in \u001b[0;36m_iLocIndexer._getitem_axis\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1749\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mTypeError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124mCannot index by location index with a non-integer key\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n\u001b[1;32m   1751\u001b[0m \u001b[38;5;66;03m# validate the location\u001b[39;00m\n\u001b[0;32m-> 1752\u001b[0m \u001b[38;5;28;43mself\u001b[39;49m\u001b[38;5;241;43m.\u001b[39;49m\u001b[43m_validate_integer\u001b[49m\u001b[43m(\u001b[49m\u001b[43mkey\u001b[49m\u001b[43m,\u001b[49m\u001b[43m \u001b[49m\u001b[43maxis\u001b[49m\u001b[43m)\u001b[49m\n\u001b[1;32m   1754\u001b[0m \u001b[38;5;28;01mreturn\u001b[39;00m \u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_ixs(key, axis\u001b[38;5;241m=\u001b[39maxis)\n",
      "File \u001b[0;32m~/anaconda3/envs/Qwen-Py310/lib/python3.10/site-packages/pandas/core/indexing.py:1685\u001b[0m, in \u001b[0;36m_iLocIndexer._validate_integer\u001b[0;34m(self, key, axis)\u001b[0m\n\u001b[1;32m   1683\u001b[0m len_axis \u001b[38;5;241m=\u001b[39m \u001b[38;5;28mlen\u001b[39m(\u001b[38;5;28mself\u001b[39m\u001b[38;5;241m.\u001b[39mobj\u001b[38;5;241m.\u001b[39m_get_axis(axis))\n\u001b[1;32m   1684\u001b[0m \u001b[38;5;28;01mif\u001b[39;00m key \u001b[38;5;241m>\u001b[39m\u001b[38;5;241m=\u001b[39m len_axis \u001b[38;5;129;01mor\u001b[39;00m key \u001b[38;5;241m<\u001b[39m \u001b[38;5;241m-\u001b[39mlen_axis:\n\u001b[0;32m-> 1685\u001b[0m     \u001b[38;5;28;01mraise\u001b[39;00m \u001b[38;5;167;01mIndexError\u001b[39;00m(\u001b[38;5;124m\"\u001b[39m\u001b[38;5;124msingle positional indexer is out-of-bounds\u001b[39m\u001b[38;5;124m\"\u001b[39m)\n",
      "\u001b[0;31mIndexError\u001b[0m: single positional indexer is out-of-bounds"
     ]
    }
   ],
   "source": [
    "import matplotlib.pyplot as plt\n",
    "\n",
    "# 初始化环境与模型\n",
    "env = MetaCIOEnv(df.tail(50).reset_index(drop=True), advisors, cached_obs=cached_obs)\n",
    "model = PPO.load(\"meta_cio_rl_cached\", env=env)\n",
    "\n",
    "# 初始资金\n",
    "initial_cash = 1000.0\n",
    "cash = initial_cash\n",
    "shares = 0  # 当前持有股票数量\n",
    "\n",
    "# 重置环境\n",
    "obs = env.reset()\n",
    "done = False\n",
    "\n",
    "# 记录曲线数据\n",
    "portfolio_values = []\n",
    "prices = []\n",
    "steps = []\n",
    "rewards = []\n",
    "\n",
    "while not done:\n",
    "    row = env.data.iloc[env.current_step]\n",
    "    price = row[\"close\"]\n",
    "\n",
    "    action, _ = model.predict(obs)\n",
    "    obs, reward, done, _ = env.step(action)\n",
    "\n",
    "    # 全部买入\n",
    "    if action == 0 and shares == 0:\n",
    "        shares = int(cash // price)  # 全部买入\n",
    "        cash -= shares * price\n",
    "\n",
    "    # 全部卖出\n",
    "    elif action == 2 and shares > 0:\n",
    "        cash += shares * price\n",
    "        shares = 0\n",
    "\n",
    "    current_value = cash + shares * price\n",
    "\n",
    "    rewards.append(reward)\n",
    "    portfolio_values.append(current_value)\n",
    "    prices.append(price)\n",
    "    steps.append(env.current_step)\n",
    "\n",
    "    env.render()\n",
    "    print(action)\n",
    "\n",
    "# 输出最终收益\n",
    "final_value = portfolio_values[-1]\n",
    "total_profit = final_value - initial_cash\n",
    "print(\"Final Portfolio Value: $\", round(final_value, 2))\n",
    "print(\"Total Profit: $\", round(total_profit, 2))\n",
    "\n",
    "# 绘图\n",
    "plt.figure(figsize=(12, 6))\n",
    "plt.plot(steps, portfolio_values, label='Portfolio Value ($)', linewidth=2)\n",
    "plt.plot(steps, prices, label='Price ($)', linestyle='--', alpha=0.7)\n",
    "plt.axhline(initial_cash, color='gray', linestyle=':', label='Initial Cash ($1000)')\n",
    "plt.title('RL Trading Performance (Full Buy/Sell)')\n",
    "plt.xlabel('Step')\n",
    "plt.ylabel('Value')\n",
    "plt.legend()\n",
    "plt.grid(True)\n",
    "plt.tight_layout()\n",
    "plt.show()\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Qwen-Py310",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
