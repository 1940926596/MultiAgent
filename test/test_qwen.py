from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

# æ›¿æ¢æˆæœ¬åœ°æ¨¡å‹è·¯å¾„
model_path = "../models/Qwen-4B"

# åŠ è½½ tokenizer å’Œæ¨¡å‹
tokenizer = AutoTokenizer.from_pretrained(model_path, trust_remote_code=True)
model = AutoModelForCausalLM.from_pretrained(model_path, device_map="auto", trust_remote_code=True)

# ç¼–å†™æç¤ºè¯­
prompt = "è¯·ä½ ä½œä¸ºä¸€ä¸ªé‡‘èåˆ†æå¸ˆï¼Œåˆ†æå½“å‰å¸‚åœºè¶‹åŠ¿ã€‚"

# æ„é€ è¾“å…¥
inputs = tokenizer(prompt, return_tensors="pt").to(model.device)

# ç”Ÿæˆè¾“å‡º
with torch.no_grad():
    outputs = model.generate(**inputs, max_new_tokens=256)

# æ‰“å°ç»“æœ
response = tokenizer.decode(outputs[0], skip_special_tokens=True)
print("\nğŸ¤– æ¨¡å‹å›ç­”ï¼š\n")
print(response)

with open('./text/test_qwen.txt', 'w', encoding='utf-8') as f:
    f.write(response)